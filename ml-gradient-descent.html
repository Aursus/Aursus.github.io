<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>BME | Machine Learning - Convex Optimization | Aursus</title><meta name="keywords" content="Major"><meta name="author" content="Aursus"><meta name="copyright" content="Aursus"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Purpose: To learn effective methods for optimizing objectives - gradient descent. This method is based on a convex function. Convex Optimization The objective of this section is to solve the optimizat">
<meta property="og:type" content="article">
<meta property="og:title" content="BME | Machine Learning - Convex Optimization">
<meta property="og:url" content="https://aursus.github.io/en/ml-gradient-descent.html">
<meta property="og:site_name" content="Aursus">
<meta property="og:description" content="Purpose: To learn effective methods for optimizing objectives - gradient descent. This method is based on a convex function. Convex Optimization The objective of this section is to solve the optimizat">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2023-12-22T18:10:50.000Z">
<meta property="article:modified_time" content="2024-01-06T04:27:22.014Z">
<meta property="article:author" content="Aursus">
<meta property="article:tag" content="Major">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/en/img/icon.png"><link rel="canonical" href="https://aursus.github.io/en/ml-gradient-descent"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/en/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/en/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"中"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BME | Machine Learning - Convex Optimization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-05 23:27:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/en/img/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/en/">Aursus</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BME | Machine Learning - Convex Optimization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-22T18:10:50.000Z" title="Created 2023-12-22 13:10:50">2023-12-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-01-06T04:27:22.014Z" title="Updated 2024-01-05 23:27:22">2024-01-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Biomedical-Engineering/">Biomedical Engineering</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Biomedical-Engineering/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Purpose: To learn effective methods for optimizing objectives - gradient descent. This method is based on a convex function.</p>
<h2 id="Convex-Optimization">Convex Optimization</h2>
<p>The objective of this section is to solve the optimization problem below:</p>
<p><img src="ml-gradient-descent/image_-o8E4aYemf.png" alt="Convex Optimization Problem"></p>
<p>Where $\mathscr{C}\subset \mathbb{R}^d$, $F:\mathbb{R}^d \rightarrow \mathbb{R}$. C is a convex set, and F is a convex function.</p>
<h3 id="Definitions-of-Convex-Function-and-Convex-Set">Definitions of Convex Function and Convex Set</h3>
<p>Definition of Convex Function:</p>
<p><img src="ml-gradient-descent/image_aobdpC89A9.png" alt="Convex Function Definition"></p>
<p>Definition of Convex Set:</p>
<p><img src="ml-gradient-descent/image_1G0XZ2cTM9.png" alt="Convex Set Definition"></p>
<p>Examples of Convex Functions:</p>
<ul>
<li>$l_2$-norm: $F(w) = |x|_2^2=x^\top x$</li>
<li>Logistic function: $F(w;x,y)=\log(1+\exp(-yw^\top x))$</li>
<li>Mean of Convex Functions: $F(w) = \frac{1}{m}\sum_{i=1}^mF_i(w)$</li>
</ul>
<h3 id="First-and-Second-Order-of-Convex-Function">First and Second-Order of Convex Function</h3>
<h4 id="First-Order">First Order</h4>
<p>For all $\omega, \omega’ \in \mathbb{R}^d$,</p>
<p>$$<br>
F(\omega’)\geq F(\omega)+\nabla F(\omega)^\top (\omega’-\omega)<br>
$$</p>
<p>Explanation: The function is above the tangent line at any point.</p>
<h4 id="Second-Order-Commonly-used-for-determining-convex-function">Second Order (Commonly used for determining convex function)</h4>
<p>For all $\omega \in \mathbb{R}^d$,</p>
<p>$$<br>
\nabla^2F(\omega)\succeq0<br>
$$</p>
<p>Or, the Hessian matrix $\nabla^2F(\omega)$ is semi-definite (PSD).</p>
<p>Explanation: The curvature of the function is always non-negative.</p>
<h3 id="Theorem">Theorem</h3>
<p>When $w$ satisfies $\nabla F(\omega)=0$, $w$ is the global minimum of $F$.</p>
<p>Proof:</p>
<p>$$<br>
F(w’)\geq F(w)+\nabla F(w)^\top(w’-w)\Rightarrow F(w’)\geq F(w)<br>
$$</p>
<h3 id="Three-Bounds-of-Convex-Functions">Three Bounds of Convex Functions</h3>
<ul>
<li>
<p>L-smooth Function (Upper Bound)</p>
<p>For all $w, w’ \in \mathbb{R}^d$,<br>
$$<br>
F(w’) \leq F(w)+\nabla F(w)^\top(w’-w)+\frac{L}{2}|w-w’|^2_2<br>
$$</p>
</li>
<li>
<p>Convex Function (Lower Bound)</p>
<p>For all $w, w’ \in \mathbb{R}^d$,<br>
$$<br>
F(w’) \geq F(w)+\nabla F(w)^\top(w’-w)<br>
$$</p>
</li>
<li>
<p>$\mu$ - Strong Convex Function (Lower Bound)</p>
<p>For all $w, w’ \in \mathbb{R}^d$,<br>
$$<br>
F(w’) \geq F(w)+\nabla F(w)^\top(w’-w)+\frac{\mu}{2}|w-w’|^2_2<br>
$$</p>
</li>
</ul>
<p><img src="ml-gradient-descent/image_IFQP4AphmX.png" alt="Convex Function Bounds"></p>
<h2 id="Gradient-Descent">Gradient Descent</h2>
<p><strong>Algorithm Itself</strong> (GD)</p>
<p><img src="ml-gradient-descent/image_zzdt45hUCB.png" alt="Gradient Descent Algorithm"></p>
<p>Where $\eta_t$ is called the learning rate, indicating how much to move each time. The stopping condition is generally set as $F(\omega_t)\leq \epsilon$ or $|\nabla F(\omega_t)|_2\leq \epsilon$.</p>
<p><strong>Selecting Learning Rate / Step Size</strong></p>
<p>Too large a learning rate can cause the results to diverge or even produce negative progress.</p>
<p>Too small a learning rate may take a long time to converge.</p>
<h3 id="Gradient-Descent-for-Smooth-Functions">Gradient Descent for Smooth Functions</h3>
<h4 id="L-Smoothness-Function">L-Smoothness Function</h4>
<p>For all $\omega, \omega’ \in \mathbb{R}^d $,</p>
<p>$$<br>
F(w’)\leq F(w)+\nabla F(w)^\top (w’-w)+\frac{L}{2}|w-w’|^2_2<br>
$$</p>
<p>Equivalent to saying $\nabla F$ is L-Lipschitz.</p>
<p>$$<br>
|\nabla F(w)-\nabla F(w’)|_2\leq L|w-w’|_2<br>
$$</p>
<h4 id="Explanation-of-Smoothness-for-Gradient-Descent">Explanation of Smoothness for Gradient Descent</h4>
<p>To minimize $F$ at $w$, we choose the minimum upper bound (L-smoothness), which is the following expression:</p>
<p>$$<br>
\min_{w’} F(w)+\nabla F(w)^\top (w’-w)+\frac{L}{2}|w’-w|_2^2<br>
$$</p>
<p>By setting the derivative to zero, we can obtain the next step $w’$, i.e.,</p>
<p>$$<br>
w’ = w-\frac{1}{L}\nabla F(w)<br>
$$</p>
<p>By comparing the GD algorithm, we can see that this corresponds to the gradient step with a learning rate $ \eta = 1/L $.</p>
<p>That is, according to the upper bound, the $w’$ obtained for the minimum upper bound corresponds to the lowest point of the upper bound, and its corresponding $F(w’)$ must be smaller than $F(w)$, thereby achieving gradient descent.</p>
<p><img src="ml-gradient-descent/image_eNrwwjvdWn.png" alt="Smoothness in Gradient Descent"></p>
<h4 id="Convergence-Proof">Convergence Proof</h4>
<p><strong>Theorem</strong></p>
<p><img src="ml-gradient-descent/image_0d5IQ2XfFA.png" alt=""></p>
<p><strong>Explanation</strong></p>
<p>Assuming at the beginning $w_1=0$ and $|\omega_*|_2=1$, then after T iterations, we have</p>
<p>$$<br>
F(w_{T+1})-F(\omega_*)\leq \frac{L}{2T}<br>
$$</p>
<p>This indicates that after $T = \frac{L}{2\epsilon}=O(1/\epsilon)$ steps, we can achieve $F(w_{T+1})-F(w_*)\leq \epsilon$. Hence, we say that gradient descent has a convergence rate of $O(1/T)$.</p>
<p><strong>Proof</strong></p>
<p><img src="ml-gradient-descent/image_DKMbpSrZQU.png" alt=""></p>
<p>For the specific proof, refer to the attached PDF (Lec10-2).</p>
<h3 id="Gradient-Descent-on-Smooth-and-Strongly-Convex-Functions">Gradient Descent on Smooth and Strongly Convex Functions</h3>
<p>$\mu$<strong>- Strongly Convex Function</strong></p>
<p>For all $\omega, \omega’ \in \mathbb{R}^d$,</p>
<p>Lower bound of F(w)</p>
<p>$$<br>
F(w’)\geq F(w)+\nabla F(w)^\top (w’-w)+\frac{\mu}{2}|w-w’|^2_2<br>
$$</p>
<h4 id="Convergence-Proof-2">Convergence Proof</h4>
<p><strong>Theorem</strong></p>
<p><img src="ml-gradient-descent/image_MIIyC-kCz7.png" alt=""></p>
<p><strong>Explanation</strong></p>
<p>Assuming at the beginning $w_1=0$ and $|\omega_*|_2=1$, then after T iterations, we have</p>
<p>$$<br>
|w_{T+1}-w_*|_2^2\leq (1-\frac{\mu}{L})^\top<br>
$$</p>
<p>This indicates that after $T = \frac{L}{\mu}\log(1/\epsilon)=O(\log(1/\epsilon))$ steps, we can achieve $|w_{T+1}-w_*|_2\leq \epsilon$. Hence, we say that gradient descent has a convergence rate of $O(\exp(-T))$.</p>
<h2 id="Appendix">Appendix</h2>
<h3 id="GD-converge-with-L-smooth-and-F-proof">GD converge with L smooth and F proof</h3>
<h4 id="Step-1">Step 1</h4>
<p>Based on the smoothness <a target="_blank" rel="noopener" href="https://www.wolai.com/3ijVrwgYE5e39yhcQTikXp#jDnPzsK6fagMiRLeVFpQqr" title="Equation 1">Equation 1</a>, at time t,</p>
<p>$$<br>
F(w_{t+1})-F(w_t)\leq \nabla F(w_t)^\top(w_{t+1}-w_t)+\frac{L}{2}|w_{t+1}-w_t|^2_2<br>
$$</p>
<p>Substituting $\nabla F(w_t) = L(w_t-w_{t+1})$ into Equation (4),</p>
<p>$$<br>
\begin{aligned}<br>
F(w_{t+1})-F(w_t)<br>
&amp;\leq L(w_{t}-w_{t+1})^\top(w_{t+1}-w_t)+\frac{L}{2}|w_{t+1}-w_t|^2_2\<br>
&amp;= -L(w_{t+1}-w_{t})^\top(w_{t+1}-w_t)+\frac{L}{2}|w_{t+1}-w_t|^2_2\<br>
&amp; =-L |w_{t+1}-w_t|<em>2^2+\frac{L}{2}|w</em>{t+1}-w_t|^2_2\<br>
&amp; =-\frac{L}{2}|w_{t+1}-w_t|^2_2<br>
\end{aligned}<br>
$$</p>
<h4 id="Step-2">Step 2</h4>
<p>Based on convex function <a target="_blank" rel="noopener" href="https://www.wolai.com/3ijVrwgYE5e39yhcQTikXp#bv6AcMaafy99B8mnEf2bBM" title="Equation 2">Equation 2</a> and $\nabla F(w_t) = L(w_t-w_{t+1})$, we obtain</p>
<p>$$</p>
<p>\begin{aligned}<br>
F(w_<em>) &amp;\geq F(w)+\nabla F(w)^\top(w_</em>-w)\<br>
\Rightarrow F(w_t)-F(w_<em>)&amp;\leq L(w_t-w_{t-1})^\top(w_t-w_</em>)<br>
\end{aligned}</p>
<p>$$</p>
<p>And as</p>
<p>$$</p>
<p>\begin{aligned}<br>
|w_{t+1}-w_<em>|<em>2^2&amp;=|w</em>{t+1}-w_t+w_t-w_</em>|<em>2^2\<br>
&amp; = |w</em>{t+1}-w_t|<em>2^2+|w</em>{t}-w_<em>|<em>2^2+2(w</em>{t+1}-w_t)^\top(w_t-w_</em>)<br>
\end{aligned}</p>
<p>$$</p>
<p>Substituting Equation (7) into Equation (6), we get</p>
<p>$$</p>
<p>\begin{aligned}<br>
F(w_t)-F(w_<em>)&amp;\leq \frac{L}{2}(|w_{t+1}-w_t|<em>2^2+|w_t-w</em></em>|<em>2^2-|w</em>{t+1}-w_*|_2^2)<br>
\end{aligned}</p>
<p>$$</p>
<p>Substituting Equation (5) into Equation (8)</p>
<p>$$</p>
<p>\begin{aligned}<br>
F(w_{t+1})-F(w_t)+F(w_t)-F(w_<em>)&amp;\leq \frac{L}{2}(|w_t-w_</em>|<em>2^2 - |w</em>{t+1}-w_<em>|<em>2^2)<br>
\ &amp;\leq\frac{L}{2}|w_t-w</em></em>|_2^2<br>
\end{aligned}</p>
<p>$$</p>
<h4 id="Step-3">Step 3</h4>
<p>Based on Equation 9, summing from tau+1 to tau yields</p>
<p>$$<br>
\begin{aligned}<br>
&amp;F(w_{\tau+1})-F(w_<em>)+F(w_{\tau+1})-F(w_</em>)+…+F(w_{\tau+1})-F(w_<em>) = \tau(F(w_{\tau+1})-F(w_</em>))\<br>
&amp;\leq F(w_{\tau+1})-F(w_<em>)+F(w_\tau)-F(w_</em>)+…+F(w_1)-F(w_<em>)=\sum_{t=1}^\tau(F(w_{t+1})-F(w_</em>))\<br>
&amp;\leq \frac{L}{2}(|w_\tau-w_<em>|^2_2-|w_{\tau+1}-w_</em>|<em>2^2+|w</em>{\tau-1}-w_<em>|^2_2-|w_{\tau}-w_</em>|<em>2^2)+…+|w_1-w</em><em>|<em>2^2-|w_2-w</em></em>|<em>2^2)\<br>
&amp;\leq \frac{L}{2}(|w_1-w</em><em>|<em>2^2-|w</em>{\tau+1}-w_</em>|<em>2^2)<br>
\ &amp;\leq\frac{L}{2}|w_1-w</em>*|_2^2<br>
\end{aligned}</p>
<p>$$</p>
<p>Substituting $w_1=0$ and $|w_*|_2^2=1$, after T iterations (i.e., $\tau=T$), we get</p>
<p>$$<br>
F(w_{T+1})-F(w_*)\leq \frac{L}{2T}<br>
$$</p>
<p>This indicates that after $T=\frac{L}{2\epsilon}$ iterations, function F converges, i.e., $F(w_{T+1})-F(w_*)\leq \epsilon$</p>
<h3 id="Comparison-of-Different-Gradient-Descent-Algorithms-Not-part-of-the-course-requirements">Comparison of Different Gradient Descent Algorithms (Not part of the course requirements)</h3>
<h4 id="Batch-Gradient-Descent-Standard-Gradient-Descent-Batch-GD">Batch Gradient Descent | Standard Gradient Descent | Batch GD</h4>
<p>Computes gradient using all samples every time (i=1～n)</p>
<p>$$<br>
f(x) = \frac{1}{n}\sum_{i=1}^nf_i(x)\<br>
\nabla f(x)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x)<br>
$$</p>
<p>Advantages: Accurate gradient update</p>
<p>Disadvantages: Only guarantees global optimum for convex function; slow convergence</p>
<p><img src="ml-gradient-descent/image_i-Ol-AlfQ6.png" alt=""></p>
<h4 id="Stochastic-Gradient-Descent-Stochastic-GD">Stochastic Gradient Descent | Stochastic GD</h4>
<p>Computes gradient using only one sample point every time (i is any value from 1 to n, randomly chosen)</p>
<p>$$<br>
f(x) = f_i(x)\<br>
\nabla f(x)=\nabla f_i(x)</p>
<p>$$</p>
<p>Advantages: Fast training speed</p>
<p>Disadvantages: Tends to jump around near the optimum, might not reach the optimum; limited to the vicinity of the optimal point</p>
<p><img src="ml-gradient-descent/image__-tHjPv8rn.png" alt=""></p>
<h4 id="Mini-Batch-Gradient-Descent-Mini-Batch-GD">Mini-Batch Gradient Descent | Mini-Batch GD</h4>
<p>Computes gradient using a batch of sample points every time (selects k samples randomly from n samples)</p>
<p>$$<br>
f(x) = \frac{1}{k}\sum_{i=1}^kf_i(x)\<br>
\nabla f(x)=\frac{1}{k}\sum_{i=1}^k \nabla f_i(x)</p>
<p>$$</p>
<p>Advantages: Faster training speed</p>
<p>Disadvantages: Chooses the direction of the smallest average gradient</p>
<p><img src="ml-gradient-descent/image_iFk53yxpyN.png" alt=""></p>
<hr>
<p>Note: The content in this blog is class notes shared for educational purposes only. Some images and content are sourced from textbooks, teacher materials, and the internet. If there is any infringement, please contact <a href="mailto:aursus.blog@gmail.com">aursus.blog@gmail.com</a> for removal.</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/en/tags/Major/">Major</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/en/anatomy-digestive.html"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">BME | Anatomy - Digestive System</div></div></a></div><div class="next-post pull-right"><a href="/en/ml-duality.html"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">BME | Machine Learning - Duality</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/en/img/icon.png" onerror="this.onerror=null;this.src='/en/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aursus</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Aursus" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:aursus.blog@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/yuji-han/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Convex-Optimization"><span class="toc-number">1.</span> <span class="toc-text">Convex Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definitions-of-Convex-Function-and-Convex-Set"><span class="toc-number">1.1.</span> <span class="toc-text">Definitions of Convex Function and Convex Set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#First-and-Second-Order-of-Convex-Function"><span class="toc-number">1.2.</span> <span class="toc-text">First and Second-Order of Convex Function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#First-Order"><span class="toc-number">1.2.1.</span> <span class="toc-text">First Order</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Second-Order-Commonly-used-for-determining-convex-function"><span class="toc-number">1.2.2.</span> <span class="toc-text">Second Order (Commonly used for determining convex function)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Theorem"><span class="toc-number">1.3.</span> <span class="toc-text">Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Three-Bounds-of-Convex-Functions"><span class="toc-number">1.4.</span> <span class="toc-text">Three Bounds of Convex Functions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">2.</span> <span class="toc-text">Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Descent-for-Smooth-Functions"><span class="toc-number">2.1.</span> <span class="toc-text">Gradient Descent for Smooth Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L-Smoothness-Function"><span class="toc-number">2.1.1.</span> <span class="toc-text">L-Smoothness Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-of-Smoothness-for-Gradient-Descent"><span class="toc-number">2.1.2.</span> <span class="toc-text">Explanation of Smoothness for Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convergence-Proof"><span class="toc-number">2.1.3.</span> <span class="toc-text">Convergence Proof</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Descent-on-Smooth-and-Strongly-Convex-Functions"><span class="toc-number">2.2.</span> <span class="toc-text">Gradient Descent on Smooth and Strongly Convex Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Convergence-Proof-2"><span class="toc-number">2.2.1.</span> <span class="toc-text">Convergence Proof</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Appendix"><span class="toc-number">3.</span> <span class="toc-text">Appendix</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GD-converge-with-L-smooth-and-F-proof"><span class="toc-number">3.1.</span> <span class="toc-text">GD converge with L smooth and F proof</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1"><span class="toc-number">3.1.1.</span> <span class="toc-text">Step 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2"><span class="toc-number">3.1.2.</span> <span class="toc-text">Step 2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-3"><span class="toc-number">3.1.3.</span> <span class="toc-text">Step 3</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Comparison-of-Different-Gradient-Descent-Algorithms-Not-part-of-the-course-requirements"><span class="toc-number">3.2.</span> <span class="toc-text">Comparison of Different Gradient Descent Algorithms (Not part of the course requirements)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Batch-Gradient-Descent-Standard-Gradient-Descent-Batch-GD"><span class="toc-number">3.2.1.</span> <span class="toc-text">Batch Gradient Descent | Standard Gradient Descent | Batch GD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Stochastic-Gradient-Descent-Stochastic-GD"><span class="toc-number">3.2.2.</span> <span class="toc-text">Stochastic Gradient Descent | Stochastic GD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mini-Batch-Gradient-Descent-Mini-Batch-GD"><span class="toc-number">3.2.3.</span> <span class="toc-text">Mini-Batch Gradient Descent | Mini-Batch GD</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Aursus</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between English And Simplified Chinese">EN</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/en/js/utils.js"></script><script src="/en/js/main.js"></script><script src="/en/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script data-pjax src="/en/self/btf.js"></script><script data-pjax src="/en/self/ch_en.js"></script></div></body></html>