<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>BME | Machine Learning - Support Vector Machine SVM | Aursus</title><meta name="keywords" content="Biomedical Engineering"><meta name="author" content="Aursus"><meta name="copyright" content="Aursus"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Linear SVM Set up Given n data points $(x_1, y_1), …, (x_n, y_n)$, where $y_i$ is 1 or -1, indicating which class $x_i$ belongs to. Each $x_i$ is a p-dimensional real vector. The goal of SVM is to fin">
<meta property="og:type" content="article">
<meta property="og:title" content="BME | Machine Learning - Support Vector Machine SVM">
<meta property="og:url" content="https://aursus.github.io/en/ml-svm.html">
<meta property="og:site_name" content="Aursus">
<meta property="og:description" content="Linear SVM Set up Given n data points $(x_1, y_1), …, (x_n, y_n)$, where $y_i$ is 1 or -1, indicating which class $x_i$ belongs to. Each $x_i$ is a p-dimensional real vector. The goal of SVM is to fin">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2023-12-22T18:10:11.000Z">
<meta property="article:modified_time" content="2024-01-06T04:28:26.263Z">
<meta property="article:author" content="Aursus">
<meta property="article:tag" content="Biomedical Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/en/img/icon.png"><link rel="canonical" href="https://aursus.github.io/en/ml-svm"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/en/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/en/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"中"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BME | Machine Learning - Support Vector Machine SVM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-05 23:28:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/en/img/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/en/">Aursus</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BME | Machine Learning - Support Vector Machine SVM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-22T18:10:11.000Z" title="Created 2023-12-22 13:10:11">2023-12-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-01-06T04:28:26.263Z" title="Updated 2024-01-05 23:28:26">2024-01-05</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Linear-SVM">Linear SVM</h2>
<h3 id="Set-up">Set up</h3>
<p>Given n data points $(x_1, y_1), …, (x_n, y_n)$, where $y_i$ is 1 or -1, indicating which class $x_i$ belongs to. Each $x_i$ is a p-dimensional real vector.</p>
<p>The goal of SVM is to find a maximum-margin hyperplane that separates all points $x_i$ based on $y_i=1$ and $y_i=-1$, ensuring the maximum distance from the hyperplane to the nearest points of both groups.</p>
<p>SVM Objective: Find a hyperplane that maximizes the margin.</p>
<p>Hyperplane Definition:</p>
<p>$$<br>
\omega^\top x_i + b = 0<br>
$$</p>
<p>Margin Definition: The distance of the nearest point to the hyperplane.</p>
<p>$$<br>
\gamma(w,b) = \min_{i\in[m]}\frac{|w^\top x_i + b|}{|w|_2}<br>
$$</p>
<h3 id="Hard-Margin-SVMs">Hard Margin SVMs</h3>
<p>Assumption: Data can be completely separated by hyperplane $\omega_<em>^\top x + b_</em> = 0$.</p>
<p>Objective: Find a classifier that obtains the maximum margin.</p>
<h4 id="Approach-1-Geometric-interpretation-wiki-approach">Approach 1: Geometric interpretation (wiki approach)</h4>
<p><img src="ml-svm/image_rD1a915hT9.png" alt=""></p>
<p>Firstly, two hyperplanes are created for the two groups of $x_i$ based on $y_i=1$ and $y_i=-1$:</p>
<p>For $y_i=1$:</p>
<p>Hyperplane 1: $\omega^\top x - b = 1$, includes all data points above this hyperplane.</p>
<p>For $y_i=-1$:</p>
<p>Hyperplane 2: $\omega^\top x - b = -1$, includes all data points below this hyperplane.</p>
<p>These two hyperplanes are parallel, and the maximum-margin hyperplane $\omega^\top x - b = 0$ lies between them.</p>
<p>Finally, all data points satisfy the following:</p>
<p>For $y_i=1$: $\omega^\top x_i - b \geq 1$</p>
<p>For $y_i=-1$: $\omega^\top x_i - b \leq -1$</p>
<p>Combining both, we get $y_i(\omega^\top x_i - b) \geq 1$ for all $1 \leq i \leq n$.</p>
<p>The minimization equation and conditions are then derived as follows:</p>
<p><img src="ml-svm/image_9xg6opU2g5.png" alt="Minimization equation"></p>
<p>This can also be written in the form of the sign function: $\text{sgn}(\omega^\top x - b)$.</p>
<h4 id="Approach-2-Deriving-from-formulas-classroom-approach">Approach 2: Deriving from formulas (classroom approach)</h4>
<p>The objective can be formulated mathematically as:</p>
<p><img src="ml-svm/image_fbfIrdj1dw.png" alt="Objective formula"></p>
<p>Simplified to:</p>
<p><img src="ml-svm/image_Wedcp9v1kS.png" alt="Simplified formula"></p>
<p>To ensure a unique solution despite different scales causing identical solutions, uniqueness is ensured by adding the following condition:</p>
<p>$$<br>
\min_{i\in [m]}|\omega^\top x_i + b| = 1<br>
$$</p>
<p>Substituting this condition into the original equation:</p>
<p><img src="ml-svm/image_giSuA-HCnQ.png" alt="Original equation"></p>
<p>This simplifies to a convex quadratic optimization problem, where both the objective function and constraints are convex:</p>
<p><img src="ml-svm/image_K6rN0ctxTO.png" alt="Quadratic optimization"></p>
<h3 id="Finding-omega-and-b">Finding $\omega$ and $b$</h3>
<p>To address the equation mentioned earlier and find $w$ and $b$ that minimize the objective function, we can obtain $\omega$ through Dual formulation and then derive $b$ using Support Vectors.</p>
<h4 id="Obtaining-omega-through-Dual-formulation">Obtaining $\omega$ through Dual formulation</h4>
<p>Initialization and approach:</p>
<p><img src="ml-svm/image_mjYLrjhQKE.png" alt="Initialization and approach"></p>
<p>Step one involves writing the Lagrangian function (note $\alpha_i &gt; 0$):</p>
<p><img src="ml-svm/image_WflijyP-NL.png" alt="Lagrangian function"><br>
<img src="ml-svm/image_HmDJssoJK9.png" alt="Lagrangian function 2"></p>
<p>Step two involves taking partial derivatives with respect to $\omega$ and $b$, setting them to zero:</p>
<p><img src="ml-svm/image_FwIuntSckk.png" alt="Partial derivatives"></p>
<p>Step three involves substituting back into the original equation and simplifying:</p>
<p><img src="ml-svm/image_CaXZK4wCg6.png" alt="Substitution and simplification"><br>
And satisfying:</p>
<p><img src="ml-svm/image_dVgd1MM6oh.png" alt="Satisfaction"></p>
<p>Step four involves expressing the dual function:</p>
<p><img src="ml-svm/image_3Hzkxt2Mzz.png" alt="Dual function"></p>
<p>Step five involves solving the fourth step’s equation to get $\alpha$. Substituting $\alpha$ into the equation results in obtaining $\omega$:</p>
<p><img src="ml-svm/image_QEaR0TNk70.png" alt="Obtaining $mega$"></p>
<h4 id="Obtaining-b-through-Support-Vectors">Obtaining $b$ through Support Vectors</h4>
<p>Definition of support vector: Any data point where $\alpha_i &gt; 0$, precisely falling on the boundary, satisfying $1=y_i(w^\top x_i+b)$.</p>
<p>Assuming $SV = {i\in [m]: \alpha_i &gt; 0}$, which is the set of all $i$ where $\alpha_i$ is greater than 0.</p>
<p>$\omega$ evolves to the following equation:</p>
<p>$$<br>
\omega=\sum_{i\in SV}\alpha_i y_i x_i<br>
$$</p>
<p>Using the complementary slackness condition from the KKT conditions, we arrive at the following equation:</p>
<p><img src="ml-svm/image_eEutzyQ7P9.png" alt="Complementary slackness condition"></p>
<p>Hence, we obtain $b$:</p>
<p>In simpler terms: Considering a set of $\alpha &gt; 0$, each $i$ can yield a $b$ (as their corresponding $x_i$ and $y_i$ differ), taking an average to reduce noise, resulting in a more accurate $b$.</p>
<p>Result of the DUAL:</p>
<p><img src="ml-svm/image_bdYyyPE-5L.png" alt="DUAL result"></p>
<h3 id="Soft-Margin-SVMs">Soft Margin SVMs</h3>
<p>Assumption: Data cannot be entirely separated by a hyperplane.</p>
<p>Due to non-separability, $y_i(\omega^\top x_i+b)\geq 1$ is infeasible. Thus, we can make it feasible by adding slack variables $\xi_i$ ($\xi_i\geq 0$) to relax the constraint.</p>
<p><img src="ml-svm/image_CCP86Ro7BN.png" alt="Soft Margin SVM introduction"></p>
<p>Where:</p>
<ul>
<li>$\xi_i=0$ signifies point $i$ is correctly classified and satisfies the large margin constraint.</li>
<li>$0&lt;\xi_i&lt;1$ signifies point $i$ is correctly classified but does not meet the large margin constraint.</li>
<li>$\xi_i&gt;1$ signifies point $i$ is misclassified.</li>
</ul>
<p>To ensure $\xi_i$ is not too large, we increase the penalty in the original equation:</p>
<p><img src="ml-svm/image_o3D8ldHoMv.png" alt="Penalized equation"></p>
<p>Where $C$ controls the penalty’s size. If $C$ is large, it degrades to a hard-margin SVM.</p>
<p>The above equation is still a convex quadratic optimization problem. We can rewrite it as the dual:</p>
<p><img src="ml-svm/image_-JiA9vt9Kq.png" alt="Convex quadratic optimization problem"></p>
<p>Following steps similar to the hard margin SVM, we can solve using the dual.</p>
<h4 id="Viewing-from-a-Loss-Minimization-Perspective">Viewing from a Loss Minimization Perspective</h4>
<p>Soft-SVM can also be written in the form of minimizing hinge loss:</p>
<p>$$<br>
l_{hinge}(y,y)=\max(0,1-yy)=\max(0,1-y_i(\omega^\top x_i-b))<br>
$$</p>
<p>If $y_i = sgn(w^Tx_i-b)$, i.e., $x_i$ is in the correct class, the output is 0; otherwise, it’s $1-y_i(\omega^\top x_i-b)$, the distance from the point to the margin.</p>
<p>Thus, SVM can be written as (where $\lambda&gt;0$):</p>
<p>$$<br>
\min_{w,b}\frac{1}{m}\sum_{i=1}^m \max(0,1-y_i(w^\top x_i+b))+\lambda |w|^2<br>
$$</p>
<p>Upon introducing slack variables, it becomes:</p>
<p><img src="ml-svm/image_EmtNFT0sMp.png" alt="Hinge loss form with slack variables"></p>
<p>Now, if we set $C=\frac{1}{2\lambda m}$, this equation becomes the soft-SVM.</p>
<h3 id="Summary">Summary</h3>
<h4 id="Hard-Margin-SVM">Hard-Margin SVM</h4>
<p>Optimization function</p>
<p><img src="ml-svm/image_oJv6JOUAj3.png" alt=""></p>
<p>Classification vector $\omega$</p>
<p>$$<br>
\omega=\sum_{i=1}^m\alpha_iy_ix_i<br>
$$</p>
<p>Support vector</p>
<p>$$<br>
SV={i\in [m]: 0&lt;\alpha_i}<br>
$$</p>
<p>Calculation of $b$</p>
<p>$$<br>
b=y_i-\omega^\top x_i<br>
$$</p>
<p>Prediction function</p>
<p>$$<br>
\text{sign}(\omega^\top x+b)\<br>
= \text{sign}(\sum_{i=1}^m \alpha_iy_ix_i^\top x+b)<br>
$$</p>
<h4 id="Soft-Margin-SVM">Soft-Margin SVM</h4>
<p>Optimization function</p>
<p><img src="ml-svm/image_3EU48zqFgp.png" alt=""></p>
<p>Classification vector $\omega$</p>
<p>$$<br>
\omega=\sum_{i=1}^m\alpha_iy_ix_i<br>
$$</p>
<p>Support vector</p>
<p>$$<br>
SV={i\in [m]: 0&lt;\alpha_i&lt;C=\frac{1}{2n\lambda}}<br>
$$</p>
<p>Calculation of $b$</p>
<p>$$<br>
b=y_i-\omega^\top x_i<br>
$$</p>
<p>Prediction function</p>
<p>$$<br>
\text{sign}(\omega^\top x+b)\<br>
= \text{sign}(\sum_{i=1}^m \alpha_iy_ix_i^\top x+b)<br>
$$</p>
<h2 id="Non-Linear-Kernel-SVM">Non-Linear Kernel SVM</h2>
<h3 id="Concept">Concept</h3>
<p>Purpose: Address non-linearly separable data that cannot be solved simply by adding slack, such as two circles.</p>
<p>Approach: To separate, we map the data to a higher dimension using a feature map $\phi$, i.e., $x\mapsto \phi(x)$.</p>
<p>Method: Replace dot product with a non-linear kernel.</p>
<p>Example</p>
<p><img src="ml-svm/image_K5Vo0F20JM.png" alt=""></p>
<p><img src="ml-svm/image_SsXcwnNJi9.png" alt=""></p>
<h3 id="Linearizing-Data-through-phi-to-obtain-a-new-predictor-function">Linearizing Data through $\phi$ to obtain a new predictor function</h3>
<p>Using feature map $\phi$, map the training data from a non-linear input space to a linear feature space:</p>
<p>$$<br>
{(x_1,y_1),…,(x_m,y_m)} \mapsto {(\phi(x_1),y_1),…,(\phi(x_m),y_m)}<br>
$$</p>
<p><img src="ml-svm/image_z7iAUgi8gm.png" alt=""></p>
<p>Thus, we obtain a linear predictor function:</p>
<p>$$<br>
\omega^\top \phi(x)+b<br>
$$</p>
<p>Where $\phi(x)$ is as follows, and $d$ represents the dimensionality of $x$:</p>
<p>$$<br>
\phi(x) = [1,x_1,…,x_d,x_1^2,x_1x_2,…,x_d^2]^\top<br>
$$</p>
<h3 id="Solving-the-optimization-function-using-Soft-SVM’s-method">Solving the optimization function using Soft-SVM’s method</h3>
<p>Since direct solving is too high-dimensional, we use the Soft-SVM method to solve:</p>
<p><img src="ml-svm/image_fxGSksaKGd.png" alt=""></p>
<p>Equivalent to solving the inner product $\phi(x)^\top \phi(x’)$</p>
<p>We can simplify it to:</p>
<p>$$<br>
\phi(x)^\top \phi(x’) = 1+x^\top x’+(x^\top x’)^2<br>
$$</p>
<h4 id="Constructing-Kernels">Constructing Kernels</h4>
<p>To simplify the solving process, let the kernel $k\in\mathbb{R}$ be defined as:</p>
<p>$$<br>
k(x_i,x_j)=&lt;\phi(x_i),\phi(x_j)&gt;=\phi(x_i)\cdot \phi(x_j)<br>
$$</p>
<p>For $x_1,x_2,…,x_m$, the kernel matrix $K\in \mathbb{R}^{m\times m}$ composed of $k$ is:</p>
<p>$$<br>
K_{ij}=k(x_i,x_j)=&lt;\phi(x_i),\phi(x_j)&gt;<br>
$$</p>
<p>Where K is a symmetric and positive semi-definite matrix, satisfying these three conditions: 1) $K=K^\top$, 2) $x^\top K x\geq 0$, 3) all eigenvalues are non-negative.</p>
<p><strong>Common Kernels</strong></p>
<p>Linear </p>
<p>$$<br>
k(x_i,x_j)=x_i^\top x_j=x_i\cdot x_j<br>
$$</p>
<p>Polynomial (homogeneous)</p>
<p>$$<br>
k(x_i,x_j)=(x_i^\top x_j)^r=(x_i\cdot x_j)^r<br>
$$</p>
<p>Polynomial (inhomogeneous)</p>
<p>$$<br>
k(x_i,x_j)=(x_i^\top x_j+d)^r=(x_i\cdot x_j+d)^r<br>
$$</p>
<p>Gaussian/Random Radial Basis Function(RBF): for $\sigma&gt;0$</p>
<p>$$<br>
k(x,x’)=\exp(-\frac{|x-x’|^2}{2\sigma^2})<br>
$$</p>
<h4 id="Kernelization-Expressing-the-original-equations-using-k">Kernelization, Expressing the original equations using k</h4>
<ol>
<li>Prove the conclusion necessarily lies within the span of training points $\omega=\sum_{i=1}^{m}\alpha_ix_i$</li>
<li>Rewrite the algorithm and predictor in the form of $x_i^\top x_j$</li>
<li>Replace with k, transforming $x_i^\top x_j\Rightarrow \phi(x_i)^\top \phi(x_j)$ with $k(x_i,x_j)$, and replace $x_i$ with $\phi(x_i)$</li>
</ol>
<h4 id="Iterative-Solution-Comparing-SVM-and-Kernel">Iterative Solution (Comparing SVM and Kernel)</h4>
<p><strong>Soft-SVM</strong></p>
<p>Optimization function</p>
<p><img src="ml-svm/image_1XUAeb6ynM.png" alt=""></p>
<p>Classification vector $\omega$</p>
<p>$$<br>
\omega=\sum_{i=1}^m\alpha_iy_ix_i<br>
$$</p>
<p>Supply vector</p>
<p>$$<br>
SV={i\in [m]: 0&lt;\alpha_i&lt;\frac{1}{2n\lambda}}<br>
$$</p>
<p>Calculation of $b$</p>
<p>$$<br>
b=y_i-\omega^\top x_i<br>
$$</p>
<p>Prediction function</p>
<p>$$<br>
\text{sign}(\omega^\top x+b)\<br>
= \text{sign}(\sum_{i=1}^m \alpha_iy_ix_i^\top x+b)<br>
$$</p>
<p>Perceptron Algorithm</p>
<p><img src="ml-svm/image_iegu9quh0v.png" alt=""></p>
<p><strong>Kernel</strong></p>
<p>Optimization function</p>
<p><img src="ml-svm/image_i66MKy65R5.png" alt=""></p>
<p>Classification vector $\omega$</p>
<p>$$<br>
\omega=\sum_{i=1}^m\alpha_iy_i\phi(x_i)<br>
$$</p>
<p>Supply vector</p>
<p>$$<br>
SV={i\in [m]: 0&lt;\alpha_i&lt;\frac{1}{2n\lambda}}<br>
$$</p>
<p>Calculation of $b$</p>
<p>$$<br>
b=y_i-\omega^\top \phi(x_i)\<br>
=y_i-[\sum_{i=1}^m\alpha_iy_i\phi(x_i)\cdot \phi(x_j)]\<br>
= y_i-[\sum_{i=1}^m\alpha_iy_i\phi(x_i)^\top \phi(x_j)]\<br>
= y_i-[\sum_{i=1}^m\alpha_iy_ik(x_i,x_j)]\<br>
$$</p>
<p>Prediction function</p>
<p>$$<br>
\text{sign}(\omega^\top \phi(x)+b)\<br>
= \text{sign}(\sum_{i=1}^m \alpha_iy_i k(x_i,x)+b)<br>
$$</p>
<p>Perceptron Algorithm</p>
<p><img src="ml-svm/image_nO0-mldPCo.png" alt="Replace $x^Tx$ inside this with kernel" title="Replace $x^Tx$ inside this with kernel"></p>
<h4 id="Kernel-Representation-of-Rigid-Regression">Kernel Representation of Rigid Regression</h4>
<p><strong>SVM’s Rigid Regression</strong></p>
<p>Original expression for Rigid Regression</p>
<p>$$<br>
\min_w \quad \frac{1}{m}\sum_{i=1}^m(y_i-w^\top x_i)^2+\lambda|w|^2_2<br>
$$</p>
<p>Replacing $w = \sum_{i=1}^m\alpha_ix_i=X^\top \alpha$, where all elements in $XX^\top$ are the inner products $x_i^\top x_j$</p>
<p>$$<br>
\min_w \quad \frac{1}{m}|Y-XX^\top \alpha|^2+\lambda\alpha^\top X X^\top \alpha<br>
$$</p>
<p>Prediction is $w^\top x=\sum_{i=1}^m\alpha_ix_i^\top x=\alpha^\top X x$</p>
<p><strong>Kernel’s Rigid Regression</strong></p>
<p>Replacing $XX^\top$ with K, where $K_{ij}=k(x_i,x_j)$</p>
<p>$$<br>
\min_\alpha \quad \frac{1}{m}|Y-K \alpha|^2+\lambda\alpha^\top K \alpha<br>
$$</p>
<p>Prediction is $w^\top \phi(x)=\sum_{i=1}^m\alpha_ik(x_i,x)=\alpha^\top k_x$</p>
<p>where $k_x = [k(x,x_1), …, k(x,x_m)]^\top$</p>
<p>optimal $\alpha = (K+\lambda m I)^{-1}Y$</p>
<hr>
<p>Note: The content in this blog is class notes shared for educational purposes only. Some images and content are sourced from textbooks, teacher materials, and the internet. If there is any infringement, please contact <a href="mailto:aursus.blog@gmail.com">aursus.blog@gmail.com</a> for removal.</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/en/tags/Biomedical-Engineering/">Biomedical Engineering</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/en/ml-duality.html"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">BME | Machine Learning - Duality</div></div></a></div><div class="next-post pull-right"><a href="/en/ml-ica.html"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">BME | Machine Learning - Independent Component Analysis (ICA)</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/en/img/icon.png" onerror="this.onerror=null;this.src='/en/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aursus</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Aursus" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:aursus.blog@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/yuji-han/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-SVM"><span class="toc-number">1.</span> <span class="toc-text">Linear SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Set-up"><span class="toc-number">1.1.</span> <span class="toc-text">Set up</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hard-Margin-SVMs"><span class="toc-number">1.2.</span> <span class="toc-text">Hard Margin SVMs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Approach-1-Geometric-interpretation-wiki-approach"><span class="toc-number">1.2.1.</span> <span class="toc-text">Approach 1: Geometric interpretation (wiki approach)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Approach-2-Deriving-from-formulas-classroom-approach"><span class="toc-number">1.2.2.</span> <span class="toc-text">Approach 2: Deriving from formulas (classroom approach)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Finding-omega-and-b"><span class="toc-number">1.3.</span> <span class="toc-text">Finding $\omega$ and $b$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Obtaining-omega-through-Dual-formulation"><span class="toc-number">1.3.1.</span> <span class="toc-text">Obtaining $\omega$ through Dual formulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Obtaining-b-through-Support-Vectors"><span class="toc-number">1.3.2.</span> <span class="toc-text">Obtaining $b$ through Support Vectors</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Soft-Margin-SVMs"><span class="toc-number">1.4.</span> <span class="toc-text">Soft Margin SVMs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Viewing-from-a-Loss-Minimization-Perspective"><span class="toc-number">1.4.1.</span> <span class="toc-text">Viewing from a Loss Minimization Perspective</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">1.5.</span> <span class="toc-text">Summary</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hard-Margin-SVM"><span class="toc-number">1.5.1.</span> <span class="toc-text">Hard-Margin SVM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Soft-Margin-SVM"><span class="toc-number">1.5.2.</span> <span class="toc-text">Soft-Margin SVM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Non-Linear-Kernel-SVM"><span class="toc-number">2.</span> <span class="toc-text">Non-Linear Kernel SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Concept"><span class="toc-number">2.1.</span> <span class="toc-text">Concept</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linearizing-Data-through-phi-to-obtain-a-new-predictor-function"><span class="toc-number">2.2.</span> <span class="toc-text">Linearizing Data through $\phi$ to obtain a new predictor function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solving-the-optimization-function-using-Soft-SVM%E2%80%99s-method"><span class="toc-number">2.3.</span> <span class="toc-text">Solving the optimization function using Soft-SVM’s method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Constructing-Kernels"><span class="toc-number">2.3.1.</span> <span class="toc-text">Constructing Kernels</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kernelization-Expressing-the-original-equations-using-k"><span class="toc-number">2.3.2.</span> <span class="toc-text">Kernelization, Expressing the original equations using k</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Iterative-Solution-Comparing-SVM-and-Kernel"><span class="toc-number">2.3.3.</span> <span class="toc-text">Iterative Solution (Comparing SVM and Kernel)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kernel-Representation-of-Rigid-Regression"><span class="toc-number">2.3.4.</span> <span class="toc-text">Kernel Representation of Rigid Regression</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Aursus</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between English And Simplified Chinese">EN</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/en/js/utils.js"></script><script src="/en/js/main.js"></script><script src="/en/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script data-pjax src="/en/self/btf.js"></script><script data-pjax src="/en/self/ch_en.js"></script></div></body></html>