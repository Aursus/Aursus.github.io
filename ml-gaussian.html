<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>生物医学工程 | 机器学习 - Gaussian Mixture &amp; EM | Aursus</title><meta name="keywords" content="生物医学工程"><meta name="author" content="Aursus"><meta name="copyright" content="Aursus"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="GMM（Gaussian Mixture Model） 什么是高斯混合模型？ 首先我们有k个高斯模型，他们分别为$ N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)  $ 我们按照一定的比例从$N(\mu_j, \sigma_j^2)$这个高斯函数中取了比例为$\phi_j$的一些样本。 按照经验可得，$\sum">
<meta property="og:type" content="article">
<meta property="og:title" content="生物医学工程 | 机器学习 - Gaussian Mixture &amp; EM">
<meta property="og:url" content="https://aursus.github.io/ml-gaussian.html">
<meta property="og:site_name" content="Aursus">
<meta property="og:description" content="GMM（Gaussian Mixture Model） 什么是高斯混合模型？ 首先我们有k个高斯模型，他们分别为$ N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)  $ 我们按照一定的比例从$N(\mu_j, \sigma_j^2)$这个高斯函数中取了比例为$\phi_j$的一些样本。 按照经验可得，$\sum">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2023-12-06T05:06:55.000Z">
<meta property="article:modified_time" content="2023-12-06T06:10:40.302Z">
<meta property="article:author" content="Aursus">
<meta property="article:tag" content="生物医学工程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/icon.png"><link rel="canonical" href="https://aursus.github.io/ml-gaussian"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"中"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '生物医学工程 | 机器学习 - Gaussian Mixture & EM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-06 01:10:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 小小家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> 语言</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Aursus</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 小小家</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> 语言</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">生物医学工程 | 机器学习 - Gaussian Mixture &amp; EM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-06T05:06:55.000Z" title="发表于 2023-12-06 00:06:55">2023-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-06T06:10:40.302Z" title="更新于 2023-12-06 01:10:40">2023-12-06</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="GMM（Gaussian-Mixture-Model）">GMM（Gaussian Mixture Model）</h2>
<h3 id="什么是高斯混合模型？">什么是高斯混合模型？</h3>
<p>首先我们有k个高斯模型，他们分别为$ N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)  $</p>
<p>我们按照一定的比例从$N(\mu_j, \sigma_j^2)$这个高斯函数中取了比例为$\phi_j$的一些样本。</p>
<p>按照经验可得，$\sum_{j=1}^{k}\phi_j=1$，即所有比例之和为1。</p>
<p>$p(z^{(i)}=j)=\phi_j$表示在$z=j$这个cluster的概率，如果是hard assignment，则用$\mathbb{I}({z^{(i)}=j})$表示。如果在这个cluster，即则$z^{(i)}=j$，则概率为1，反之不在这个cluster则为0。</p>
<p>$p(x^{(i)}|z^{(i)}=j)=p_{norm}(x^{(i)};\mu_j,\sigma_j)$表示在$j$这个高斯分布中的取到$x$的概率，如</p>
<p>每一个$x^{(i)}$都对应k个$z^{(i)}_j$</p>
<h3 id="基本参数和分布">基本参数和分布</h3>
<p><strong>input</strong>: $$ {x^{(1)}, …, x^{(n)}}  $$</p>
<p><strong>latent variable</strong>:</p>
<p>$$ {z^{(1)}, …, z^{(n)}},\ z^{(i)}=(z^{(i)}=1,z^{(i)}=2,…,z^{(i)}=k) $$</p>
<p>换而言之，每个$z^{(i)}$都对应一个$x^{(i)}$，$z^{(i)}$满足多项式分布，且可以拆分为分属不同k的z。通过分属不同k的z来拼凑出大的z，从而得到x。</p>
<p><strong>joint distribution</strong></p>
<p>$$p(x^{(i)}, z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$$</p>
<p>其中，$z{(i)}$~ Multinomial$(\phi)$多项式分布，where $\phi_j \geq 0$，$\sum_{j=1}^{k}\phi_j=1$。</p>
<p>$$x^{(i)}|z^{(i)}=j\sim N(\mu_j, \Sigma_j)$$</p>
<p>$z^{(i)}$指示了每个$x^{(i)}$是从那个k Gaussians来的。</p>
<p>$p(z)$表示probability of choose cluster z，也就是density of mixture model</p>
<p>$p(z^{(i)}=j)$表示在z=j这个cluster的概率</p>
<p>i表示第几个data的index，x表示data</p>
<p>j表示第几个cluster的index，z表示cluster</p>
<p>参数$\phi_j$提供$p(z^{(i)}=j)$，$x^{(i)}|z^{(i)}=j\sim N(\mu_j, \Sigma_j)$，即x|z满足某一个高斯分布，N（）为高斯分布</p>
<p><strong>likelihood of data</strong></p>
<p>$$<br>
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^{n}\log p(x^{(i)};\phi, \mu, \Sigma) —(1)\<br>
= \sum_{i=1}^{n}\log\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)}=j; \mu,\Sigma)p(z^{(i)}=j;\phi)—(2)\</p>
<p>$$</p>
<p>为了解决（1），所以我们引入了z从而将这一个概率拆为两个部分的概率的相乘，从而得到（2）</p>
<p>其中<code>;</code>左侧表示变量，右侧表示参数，<code>,</code>表示并列。$x|z;\mu,\Sigma$中，$｜$的右侧表示整体范围（即$z=j$的范围内），左侧$x$表示在右侧那些范围中x的概率</p>
<p>如果$z^{(i)}$已知的话，即如果我们知道zi来自于哪个高斯分布，换句话说我们把j固定了下来，则式子可以化简为</p>
<p>$$<br>
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^{n}\log p(x^{(i)}|z^{(i)}; \mu,\Sigma)+\log p(z^{(i)};\phi)—(3)<br>
$$</p>
<p>maximum函数（3），通过求偏导=0，从而算出结果为：（此时固定了j）</p>
<p>$$\phi _ { j } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j }$$</p>
<p>$$\mu _ { j } = \frac { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } x ^ { ( i ) } } { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } }$$</p>
<p>$$\Sigma _ { j } = \frac { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } ( x ^ { ( i ) } - \mu _ { j } ) ( x ^ { ( i ) } - \mu _ { j } ) ^ { T } } { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } }$$</p>
<p>从上图可以看出来，$z^{(i)}$可以视为class label的作用，所以只要知道$z^{(i)}$就可以都求出来了，<strong>所以我们通过使用EM方法来求</strong>$z^{(i)}$</p>
<h2 id="EM-Expectation-Maximization-algorithm-for-density-estimation">EM (Expectation-Maximization) algorithm for density estimation</h2>
<p><strong>（E-Step）</strong> 猜测$z^{(i)}$，cluster assignment</p>
<p>For each $i, j$, set</p>
<p>$$w _ { j } ^ { ( i ) } : = p ( z ^ { ( i ) } = j | x ^ { ( i ) } ; \phi , \mu , \Sigma )$$</p>
<p>这个$\omega_j^{(i)}$表示我们对$z^{(i)}$的猜测。因为我们要求z，所以把z放在｜的左边</p>
<p>通过Bayes 法则，我们可以得到：</p>
<p>$$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$$</p>
<p>$$p ( z ^ { ( i ) } = j | x ^ { ( i ) } ; \phi , \mu , \Sigma ) = \frac { p ( x ^ { ( i ) } | z ^ { ( i ) } = j ; \mu, \Sigma ) p ( z ^ { ( i ) } = j ; \mu, \Sigma ) p ( z ^ { ( i ) } = j ; \phi ) } { \sum _ { l = 1 } ^ { k } p ( x ^ { ( i ) } | z ^ { ( i ) } = l ; \mu, \Sigma ) p ( z ^ { ( i ) } = l ; \phi )} $$</p>
<p>$$f(x)=\frac{1}{\sqrt{(2\pi)^k \det \Sigma}} \exp (-\frac{1}{2}(x-\mu)^{T}\Sigma ^{-1}(x-\mu))$$</p>
<p>$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$来自于density of Gaussian with mean $\mu_j$和convariance$\Sigma_j$，表示如果xi来自于高斯分布j，则从该模型中取到x的概率</p>
<p>$p(z^{(i)}=j;\phi)$=$\phi_j$，表示来自于j这个高斯模型的概率</p>
<p><strong>（M-Step）</strong> 基于猜测update model的parameter（MLE）——soft assignment</p>
<p>Update the parameters:</p>
<p>$$\phi _ { j } : = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) }$$</p>
<p>$$\mu _ { j } : = \frac { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } x ^ { ( i ) } } { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } }$$</p>
<p>$$\Sigma _ { j } : = \frac { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } ( x ^ { ( i ) } - \mu _ { j } ) ( x ^ { ( i ) } - \mu _ { j } ) ^ { T } } { \sum _ { i = 1 } ^ { n } w_j ^{(i)} }$$</p>
<p><strong>Restriction</strong></p>
<p>$$<br>
\Sigma_j=\sigma^2I<br>
$$</p>
<p>其中$\sigma^2\rightarrow 0$则原函数化简为k-means</p>
<p>从概率的角度而言，高斯图像无限逼近脉冲函数，只有靠近中心的位置才有数值，其余都很小。因此针对于某一个高斯分布，概率很大，对其他而言，概率很小。所以从w的数值而言，按照贝叶斯分布的那个分子分母公式，这个数据只有在某一个高斯分布里面才会很大，其他都无限接近于0.</p>
<p>从cluster的角度而言，sigma变为0表示GMM椭圆的cluster边界变为k-mean圆形的边界（或者说椭球形变为球型）</p>
<h3 id="K-means的进阶方法">K-means的进阶方法</h3>
<p>不再使用hard cluster assignment $c(i)$，而是使用soft assignments $\omega_j^{(i)}$</p>
<p>作业代码里面，w是mxk的矩阵，里面装着xi属于高斯分布j的概率。如果要变成hard cluster，只要遍历xi在所有k个高斯分布里面概率，找到最大概率的那个高斯分布，然后就认定是这个高斯分布，除此以外的概率都是0，所以矩阵退化为mx1的向量。</p>
<h3 id="GMM求解">GMM求解</h3>
<p>GMM可以通过run EM或者min loss function来求解</p>
<h3 id="其他">其他</h3>
<p>从cluster l中观测到x的概率：</p>
<p>$$<br>
P(x) = \sum P(x, z=l) = \sum P(x|z=l)P(z=l)<br>
$$</p>
<p>单个参数，用求导</p>
<p>多个参数，用MLE或者EM</p>
<h2 id="reference">reference</h2>
<p><a target="_blank" rel="noopener" href="https://mas-dse.github.io/DSE210/Additional%20Materials/gmm.pdf">https://mas-dse.github.io/DSE210/Additional Materials/gmm.pdf</a></p>
<p>什么是高斯混合模型</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV12d4y167ud/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV12d4y167ud/?spm_id_from=333.337.search-card.all.click</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Dd4y167pZ/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=2029748cdca0ccea25511170cf7a00c3">https://www.bilibili.com/video/BV1Dd4y167pZ/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=2029748cdca0ccea25511170cf7a00c3</a></p>
<hr>
<p>两个高斯模型组成的，选到模型1的概率为 $\pi$，选到模型2的概率为 $1-\pi$。下面就是选到了x模型的概率</p>
<p>$$p ( x ; \pi , \sigma _ { 1 } , \sigma _ { 2 } , \mu _ { 1 } , \mu _ { 2 } ) = \pi p _ { n o r m } ( x ; \mu _ { 1 } , \sigma _ { 1 } ) + ( 1 - \pi ) p _ { n o r m } ( x ; \mu _ { 2 } , \sigma _ { 2 } )$$</p>
<p>对其进行 $log$，因为 $log(x)$ 是函数，所以求 $\max x$ 和求 $\max \log(x)$ 一个道理</p>
<p>$$\log ( L ( x ; \pi , \mu _ { 1 } , \mu _ { 2 } , \sigma _ { 1 } , \sigma _ { 2 } ) ) = \sum \log ( \pi p _ { n o r m } ( x ; \mu _ { 1 } , \sigma _ { 1 } ) + ( 1 - \pi ) p _ { n o r m } ( x ; \mu _ { 2 }, \sigma_2 ) )$$</p>
<p>$$p ( x , z ) = p ( x | z ) p ( z ) $$</p>
<p>where,</p>
<p>$$z \in { 0 , 1 } , p ( z = 1 ) = \pi ,\ p ( x | z = 1 ) = p _ {norm} ( x ; \mu _ { 1 } \sigma_1),\ p ( x | z =0 ) = p _ {norm}(x; \mu_2, \sigma_2)$$</p>
<p>z和x同时发生的概率=z发生的概率*x发生的概率然后求和</p>
<p>$$ { \log ( L ( p ( z , x ; \pi , \mu _ { 1 } , \mu _ { 2 } , \sigma_1, \sigma _ { 2 } ) ) ) } \ { = \sum _ { i } z _ { i } \log ( \pi p _ { n } ( x _ { i } ; \mu _ { 1 } , \sigma _ { 1 } )) + ( 1 - z _ { i } ) \log ( ( 1 - \pi ) p_n (x_i; \mu_2,\sigma_2))}$$</p>
<p><img src="ml-gaussian/image_0TETJdB-yR.png" alt=""></p>
<p>期望=log（p（x，z））的概率乘以是否发生，也就是p（z=1｜x）</p>
<p><img src="ml-gaussian/image_ih6VNLC2Fv.png" alt=""></p>
<p><img src="ml-gaussian/image_VEzh0sPANi.png" alt=""></p>
<hr>
<p>声明：此blog内容为上课笔记，仅为分享使用。部分图片和内容取材于课本、老师课件、网络。如果有侵权，请联系aursus.blog@gmail.com删除。</p>
</article><div class="tag_share"><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/hexo-bilingual.html"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">hexo-中英双语 hexo+butterfly</div></div></a></div><div class="next-post pull-right"><a href="/ml-kmeans.html"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">生物医学工程 | 机器学习 - K means聚类</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aursus</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Aursus" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:aursus.blog@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/yuji-han/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#GMM%EF%BC%88Gaussian-Mixture-Model%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">GMM（Gaussian Mixture Model）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">什么是高斯混合模型？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E5%92%8C%E5%88%86%E5%B8%83"><span class="toc-number">1.2.</span> <span class="toc-text">基本参数和分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EM-Expectation-Maximization-algorithm-for-density-estimation"><span class="toc-number">2.</span> <span class="toc-text">EM (Expectation-Maximization) algorithm for density estimation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E7%9A%84%E8%BF%9B%E9%98%B6%E6%96%B9%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">K-means的进阶方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GMM%E6%B1%82%E8%A7%A3"><span class="toc-number">2.2.</span> <span class="toc-text">GMM求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">2.3.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">3.</span> <span class="toc-text">reference</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Aursus</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="中英转换">中</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script data-pjax src="/self/btf.js"></script><script data-pjax src="/self/ch_en.js"></script></div></body></html>