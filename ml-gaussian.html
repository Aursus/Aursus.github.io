<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>BME | Machine Learning - Gaussian Mixture &amp; EM | Aursus</title><meta name="keywords" content="Major"><meta name="author" content="Aursus"><meta name="copyright" content="Aursus"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="GMM（Gaussian Mixture Model） What is a Gaussian Mixture Model? Firstly, we have k Gaussian models, denoted as $N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)$. According to certain">
<meta property="og:type" content="article">
<meta property="og:title" content="BME | Machine Learning - Gaussian Mixture &amp; EM">
<meta property="og:url" content="https://aursus.github.io/en/ml-gaussian.html">
<meta property="og:site_name" content="Aursus">
<meta property="og:description" content="GMM（Gaussian Mixture Model） What is a Gaussian Mixture Model? Firstly, we have k Gaussian models, denoted as $N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)$. According to certain">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2023-12-06T05:06:55.000Z">
<meta property="article:modified_time" content="2024-01-06T04:27:52.222Z">
<meta property="article:author" content="Aursus">
<meta property="article:tag" content="Major">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/en/img/icon.png"><link rel="canonical" href="https://aursus.github.io/en/ml-gaussian"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/en/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/en/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"中"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BME | Machine Learning - Gaussian Mixture & EM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-05 23:27:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/en/img/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/en/">Aursus</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://aursus.github.io/"><i class="fa-fw fas fa-c"></i><span> 中文</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BME | Machine Learning - Gaussian Mixture &amp; EM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-06T05:06:55.000Z" title="Created 2023-12-06 00:06:55">2023-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-01-06T04:27:52.222Z" title="Updated 2024-01-05 23:27:52">2024-01-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Biomedical-Engineering/">Biomedical Engineering</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Biomedical-Engineering/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="GMM（Gaussian-Mixture-Model）">GMM（Gaussian Mixture Model）</h2>
<h3 id="What-is-a-Gaussian-Mixture-Model">What is a Gaussian Mixture Model?</h3>
<p>Firstly, we have k Gaussian models, denoted as $N(\mu_1, \sigma_1^2), N(\mu_2, \sigma_2^2), …, N(\mu_k, \sigma_k^2)$.</p>
<p>According to certain proportions, we sampled some data from the Gaussian function $N(\mu_j, \sigma_j^2)$ with proportions $\phi_j$.</p>
<p>Empirically, it can be observed that $\sum_{j=1}^{k}\phi_j=1$, which means the sum of all proportions is 1.</p>
<p>$p(z^{(i)}=j)=\phi_j$ represents the probability of being in cluster $z=j$. If it is a hard assignment, it is represented by $\mathbb{I}({z^{(i)}=j})$. If $z^{(i)}=j$ in this cluster, the probability is 1; otherwise, it is 0 if not in this cluster.</p>
<p>$p(x^{(i)}|z^{(i)}=j)=p_{norm}(x^{(i)};\mu_j,\sigma_j)$ represents the probability of obtaining $x$ from Gaussian distribution $j$, such as:</p>
<p>Each $x^{(i)}$ corresponds to k $z^{(i)}_j$.</p>
<h3 id="Basic-parameters-and-distributions">Basic parameters and distributions</h3>
<p><strong>input</strong>: $$ {x^{(1)}, …, x^{(n)}}  $$</p>
<p><strong>latent variable</strong>:</p>
<p>$$ {z^{(1)}, …, z^{(n)}},\ z^{(i)}=(z^{(i)}=1,z^{(i)}=2,…,z^{(i)}=k) $$</p>
<p>In other words, each $z^{(i)}$ corresponds to an $x^{(i)}$. $z^{(i)}$ follows a multinomial distribution and can be divided into different z belonging to different k’s. By belonging to different z of k, larger z can be pieced together to obtain x.</p>
<p><strong>joint distribution</strong></p>
<p>$$p(x^{(i)}, z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$$</p>
<p>Where $z^{(i)} \sim \text{Multinomial}(\phi)$, a multinomial distribution where $\phi_j \geq 0$, and $\sum_{j=1}^{k}\phi_j=1$.</p>
<p>$$x^{(i)}|z^{(i)}=j\sim N(\mu_j, \Sigma_j)$$</p>
<p>$z^{(i)}$ indicates which of the k Gaussians each $x^{(i)}$ comes from.</p>
<p>$p(z)$ represents the probability of choosing cluster z, which is the density of the mixture model.</p>
<p>$p(z^{(i)}=j)$ represents the probability in cluster z=j.</p>
<p>Here, i denotes the index of the data, x represents the data itself, j denotes the index of the cluster, and z represents the cluster.</p>
<p>The parameter $\phi_j$ provides $p(z^{(i)}=j)$, $x^{(i)}|z^{(i)}=j \sim N(\mu_j, \Sigma_j)$, meaning that x|z follows a certain Gaussian distribution, where N() represents the Gaussian distribution.</p>
<p><strong>likelihood of data</strong></p>
<p>$$<br>
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^{n}\log p(x^{(i)};\phi, \mu, \Sigma) —(1)\<br>
= \sum_{i=1}^{n}\log\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)}=j; \mu,\Sigma)p(z^{(i)}=j;\phi)—(2)\</p>
<p>$$</p>
<p>To address (1), we introduce z, thereby breaking down the probability into two parts multiplied together, resulting in (2).</p>
<p>Wherein the ; on the left side represents the variable and on the right side represents the parameter, while , denotes parallelism. In $x|z;\mu,\Sigma$, the right side of | represents the overall range (i.e., the range where $z=j$), and the left side, x, signifies the probability of x within those specified ranges.</p>
<p>If $z^{(i)}$ is known, that is, if we know which Gaussian distribution zi comes from—put differently, when j is fixed—the equation can be simplified to:</p>
<p>$$<br>
\ell(\phi, \mu, \Sigma) = \sum_{i=1}^{n}\log p(x^{(i)}|z^{(i)}; \mu,\Sigma)+\log p(z^{(i)};\phi)—(3)<br>
$$</p>
<p>The maximum function (3) is obtained by taking partial derivatives and setting them to zero, resulting in the calculation of the result: (at this point, j is fixed)</p>
<p>$$\phi _ { j } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j }$$</p>
<p>$$\mu _ { j } = \frac { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } x ^ { ( i ) } } { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } }$$</p>
<p>$$\Sigma _ { j } = \frac { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } ( x ^ { ( i ) } - \mu _ { j } ) ( x ^ { ( i ) } - \mu _ { j } ) ^ { T } } { \sum _ { i = 1 } ^ { n } 1 { z ^ { ( i ) } = j } }$$</p>
<p>From the above figure, it can be observed that $z^{(i)}$ can be considered as the role of a class label. Therefore, once $z^{(i)}$ is known, everything else can be derived. Thus, we use the Expectation-Maximization (EM) method to determine $z^{(i)}$.</p>
<h2 id="EM-Expectation-Maximization-algorithm-for-density-estimation">EM (Expectation-Maximization) algorithm for density estimation</h2>
<p><strong>（E-Step）</strong> Gaussing$z^{(i)}$，cluster assignment</p>
<p>For each $i, j$, set</p>
<p>$$w _ { j } ^ { ( i ) } : = p ( z ^ { ( i ) } = j | x ^ { ( i ) } ; \phi , \mu , \Sigma )$$</p>
<p>This $\omega_j^{(i)}$ represents our guess for $z^{(i)}$. Because we are trying to determine z, it’s placed on the left side of |.</p>
<p>Using Bayes’ rule, we can obtain:</p>
<p>$$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$$</p>
<p>$$p ( z ^ { ( i ) } = j | x ^ { ( i ) } ; \phi , \mu , \Sigma ) = \frac { p ( x ^ { ( i ) } | z ^ { ( i ) } = j ; \mu, \Sigma ) p ( z ^ { ( i ) } = j ; \mu, \Sigma ) p ( z ^ { ( i ) } = j ; \phi ) } { \sum _ { l = 1 } ^ { k } p ( x ^ { ( i ) } | z ^ { ( i ) } = l ; \mu, \Sigma ) p ( z ^ { ( i ) } = l ; \phi )} $$</p>
<p>$$f(x)=\frac{1}{\sqrt{(2\pi)^k \det \Sigma}} \exp (-\frac{1}{2}(x-\mu)^{T}\Sigma ^{-1}(x-\mu))$$</p>
<p>$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$ comes from the density of the Gaussian distribution with mean $\mu_j$ and covariance $\Sigma_j$. It represents the probability of obtaining x from the model if $x^{(i)}$ comes from Gaussian distribution j.</p>
<p>$p(z^{(i)}=j;\phi)=\phi_j$ represents the probability of coming from Gaussian model j.</p>
<p><strong>(M-Step)</strong> Update the model’s parameters based on the guess (Maximum Likelihood Estimation - MLE) — soft assignment.<br>
Update the parameters:</p>
<p>$$\phi _ { j } : = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) }$$</p>
<p>$$\mu _ { j } : = \frac { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } x ^ { ( i ) } } { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } }$$</p>
<p>$$\Sigma _ { j } : = \frac { \sum _ { i = 1 } ^ { n } w _ { j } ^ { ( i ) } ( x ^ { ( i ) } - \mu _ { j } ) ( x ^ { ( i ) } - \mu _ { j } ) ^ { T } } { \sum _ { i = 1 } ^ { n } w_j ^{(i)} }$$</p>
<p><strong>Restriction</strong></p>
<p>$$<br>
\Sigma_j=\sigma^2I<br>
$$</p>
<p>Where $\sigma^2\rightarrow 0$, the original function simplifies to k-means.</p>
<p>From a probabilistic perspective, a Gaussian function approaches an impulse function as $\sigma^2$ approaches zero. Only positions near the center have significant values; the rest become very small. Thus, for a specific Gaussian distribution, the probability is high, while for others, it is very low. Therefore, concerning the values of w, according to the Bayes’ formula, this data will have high values only within a specific Gaussian distribution, whereas the others tend towards zero.</p>
<p>From a clustering perspective, when sigma approaches zero, it means the boundaries of the ellipsoidal clusters in GMM become circular in k-means (or in other words, ellipsoids become spherical).</p>
<h3 id="Advanced-methods-in-K-means">Advanced methods in K-means</h3>
<p>Instead of using hard cluster assignment $c(i)$, we’re using soft assignments $\omega_j^{(i)}$.</p>
<p>In the code, w is an m x k matrix containing the probabilities of $x_i$ belonging to Gaussian distribution j. If we want to convert this into hard clustering, we iterate through the probabilities of $x_i$ across all k Gaussian distributions and find the Gaussian distribution with the highest probability. Then, we assign $x_i$ to that Gaussian distribution, setting all other probabilities to 0. Consequently, the matrix is reduced to an m x 1 vector.</p>
<h3 id="GMM-Solving">GMM Solving</h3>
<p>GMM can be solved using EM or by minimizing the loss function.</p>
<h3 id="Other">Other</h3>
<p>The probability of observing x from cluster l:</p>
<p>$$<br>
P(x) = \sum P(x, z=l) = \sum P(x|z=l)P(z=l)<br>
$$</p>
<h2 id="reference">reference</h2>
<p><a target="_blank" rel="noopener" href="https://mas-dse.github.io/DSE210/Additional%20Materials/gmm.pdf">https://mas-dse.github.io/DSE210/Additional Materials/gmm.pdf</a></p>
<p>什么是高斯混合模型</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV12d4y167ud/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV12d4y167ud/?spm_id_from=333.337.search-card.all.click</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Dd4y167pZ/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=2029748cdca0ccea25511170cf7a00c3">https://www.bilibili.com/video/BV1Dd4y167pZ/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=2029748cdca0ccea25511170cf7a00c3</a></p>
<hr>
<p>The probability of selecting model 1 is $\pi$, and the probability of selecting model 2 is $1-\pi$. Below is the probability of selecting model x.</p>
<p>$$p ( x ; \pi , \sigma _ { 1 } , \sigma _ { 2 } , \mu _ { 1 } , \mu _ { 2 } ) = \pi p _ { n o r m } ( x ; \mu _ { 1 } , \sigma _ { 1 } ) + ( 1 - \pi ) p _ { n o r m } ( x ; \mu _ { 2 } , \sigma _ { 2 } )$$</p>
<p>Taking the logarithm of it, because $\log(x)$ is a function, maximizing $x$ is equivalent to maximizing $\log(x)$.</p>
<p>$$\log ( L ( x ; \pi , \mu _ { 1 } , \mu _ { 2 } , \sigma _ { 1 } , \sigma _ { 2 } ) ) = \sum \log ( \pi p _ { n o r m } ( x ; \mu _ { 1 } , \sigma _ { 1 } ) + ( 1 - \pi ) p _ { n o r m } ( x ; \mu _ { 2 }, \sigma_2 ) )$$</p>
<p>$$p ( x , z ) = p ( x | z ) p ( z ) $$</p>
<p>where,</p>
<p>$$z \in { 0 , 1 } , p ( z = 1 ) = \pi ,\ p ( x | z = 1 ) = p _ {norm} ( x ; \mu _ { 1 } \sigma_1),\ p ( x | z =0 ) = p _ {norm}(x; \mu_2, \sigma_2)$$</p>
<p>The probability of both z and x occurring simultaneously equals the probability of z occurring multiplied by the probability of x occurring, and then summed over all possible scenarios.</p>
<p>$$ { \log ( L ( p ( z , x ; \pi , \mu _ { 1 } , \mu _ { 2 } , \sigma_1, \sigma _ { 2 } ) ) ) } \ { = \sum _ { i } z _ { i } \log ( \pi p _ { n } ( x _ { i } ; \mu _ { 1 } , \sigma _ { 1 } )) + ( 1 - z _ { i } ) \log ( ( 1 - \pi ) p_n (x_i; \mu_2,\sigma_2))}$$</p>
<p><img src="ml-gaussian/image_0TETJdB-yR.png" alt=""></p>
<p>The expectation of $\log(p(x,z))$ is the probability multiplied by whether it happens, i.e., $p(z=1|x)$.</p>
<p><img src="ml-gaussian/image_ih6VNLC2Fv.png" alt=""></p>
<p><img src="ml-gaussian/image_VEzh0sPANi.png" alt=""></p>
<hr>
<p>Announcement：This blog content serves as class notes and is solely for sharing purposes. Some images and content are sourced from textbooks, teacher presentations, and the internet. If there are any copyright infringements, please contact <a href="mailto:aursus.blog@gmail.com">aursus.blog@gmail.com</a> for removal.</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/en/tags/Major/">Major</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/en/hexo-bilingual.html"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">hexo - Bilingual hexo + butterfly</div></div></a></div><div class="next-post pull-right"><a href="/en/ml-kmeans.html"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">BME | Machine Learning - K means Clustering</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/en/img/icon.png" onerror="this.onerror=null;this.src='/en/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aursus</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Aursus" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:aursus.blog@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.com/in/yuji-han/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#GMM%EF%BC%88Gaussian-Mixture-Model%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">GMM（Gaussian Mixture Model）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-a-Gaussian-Mixture-Model"><span class="toc-number">1.1.</span> <span class="toc-text">What is a Gaussian Mixture Model?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-parameters-and-distributions"><span class="toc-number">1.2.</span> <span class="toc-text">Basic parameters and distributions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EM-Expectation-Maximization-algorithm-for-density-estimation"><span class="toc-number">2.</span> <span class="toc-text">EM (Expectation-Maximization) algorithm for density estimation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-methods-in-K-means"><span class="toc-number">2.1.</span> <span class="toc-text">Advanced methods in K-means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GMM-Solving"><span class="toc-number">2.2.</span> <span class="toc-text">GMM Solving</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other"><span class="toc-number">2.3.</span> <span class="toc-text">Other</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">3.</span> <span class="toc-text">reference</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Aursus</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between English And Simplified Chinese">EN</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/en/js/utils.js"></script><script src="/en/js/main.js"></script><script src="/en/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script data-pjax src="/en/self/btf.js"></script><script data-pjax src="/en/self/ch_en.js"></script></div></body></html>